{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Process (Finite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A countable MDP is defined as triplet $ \\mathcal {M = \\{X, Y, P_0\\}} $ where $ \\mathcal X$ is countable nonempty set of states, $\\mathcal A$ is counatble non empty set of actions and A transition probabilty kernel $ \\mathcal P_0 $ assigns a probability to each action state pair $(x,a) \\in \\mathcal {X*A} $ a probability measure $ \\mathcal {X*R} $, which can be denoted as $ \\mathcal {P_0(.|x,a)} $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A finite MDP is defined by its set of states and set of actions and one step dynamics given by transition probability \n",
    "$$ p(s'|s,a) = Pr(S_{t+1} = s'| S_t = s, A_t = a) $$\n",
    "similarly given any state s and action a, expected value of reward of next period is given as\n",
    "$$ r(s,a, s') = E\\{R_{t+1}|S_t=s, A_t=a, S_{t+1}=s'\\} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A backward dynamic programming algorithm\n",
    "\n",
    "Step 0: Initilization\n",
    "        Initialize the terminal rewards $ V_T(S_T) $.\n",
    "\n",
    "Step 1: Calculate:\n",
    "        $$ V_t(S_t) = \\max_{a_t \\in A} \\{ C_t(s_t, a_t) + \\gamma \\sum_{s' \\in S} Pr(s'|S_t, a_t)V_{t+1}(s) \\} $$.\n",
    "        \n",
    "Step 2: Repeat step 1 for all values of $ t \\ge 0 $\n",
    "\n",
    "\n",
    "### Infinite Horizon Problem \n",
    "In the optimality equation \n",
    "$$ V_t(S_t) = \\max_{a_t} \\{ C_t(s_t, a_t) + \\gamma \\sum_{s' \\in S} Pr(s'|S_t, a_t)V_{t+1}(s') \\} $$.\n",
    "We can think of steady state problem as one without time dimnsion and letting $ V(S) = \\lim_{t \\to \\infty} V_t(S_t)$ and we obtain steady state optimality equation\n",
    "$$ V(S) = \\max_{a \\in A} \\{ C(s, a) + \\gamma \\sum_{s' \\in S} Pr(s'|s, a)V(s') \\} $$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value iteration algorithm for infinite horizon problem\n",
    "\n",
    "Step 0: Initialize $V^0(s)  = 0  \\forall s \\in S $.\n",
    "        Fix tolerance parameter $\\epsilon > 0 $.\n",
    "        Set n=1.\n",
    "\n",
    "Step 2: For each s in S Calculate \n",
    "        $$ V^n(S) = \\max_{a \\in A} \\{ C(s, a) + \\gamma \\sum_{s' \\in S} Pr(s'|s, a)V^{n-1}(s') \\} $$.\n",
    "        Let x be the decision vector that solves above equation.\n",
    "\n",
    "Step 3: If $ V^n - V^{n-1} \\le \\frac{\\epsilon (1 - \\gamma)}{2\\epsilon} \\forall s \\in S $ then stop else set n = n+1 and repeat from step 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximate Dynamic Programming Algo using one step transition\n",
    "\n",
    "Stpe 0 :\n",
    "        0.a : Initizalize $V_t(s_t) $ fro all state s_t.\n",
    "        \n",
    "        0.b : Choose initial value s_0^1.\n",
    "        \n",
    "        0.c : Set n =1\n",
    "\n",
    "Step 1: Choose a sample path $\\omega^n$\n",
    "\n",
    "Step 2:\n",
    "\n",
    "Step 2.a :\n",
    "        for t=0,1,2,3,...,T Solve\n",
    "        $$ v_t^n  = \\max_{a_t \\in A_t} \\{ C_t(s_t, a_t) + \\gamma \\sum_{s' \\in S} \\} Pr(s'|s_t^n, a_t) V_{t+1}^{n-1}(s')\\} $$\n",
    "        let $x_t^n$ will be the value of $x_t$ that solves above.\n",
    "\n",
    "Step 2.b : Update $ V_{t}^{n-1}(S_t) $ using  $ V_t^n(S_t) = v_t^n $  if $ S_t = S_t^n $ otherwise  $V_t^n(S_t) = V_t^{n-1}(S_t) $ .\n",
    "\n",
    "Step 2.c : Compute $S_{t+1}^n$ using tansition dynamics.  \n",
    "\n",
    "Step 3: increment n, check if it is less than N, go to step 1, else stop.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Above algorithms for stock selling under following dynamics\n",
    "\n",
    "Let the price changes as follows\n",
    "$$ p_t = p_{t-1}(1- h_1(x_{t-1}) + h_2(\\epsilon_t)) $$\n",
    "where $\\epsilon_t, x_t $ are normally distributed iid random variable and stocks applied to sell in market respectively.\n",
    "\n",
    "The reward function is defined as \n",
    "$$ c(p_t, x_t) = x_t*p_t(1-h_1(x_t)) $$\n",
    "\n",
    "Let us take $ T = 10 $ and $N = 10000 $ and value of parameter $\\eta = 0.0001 $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 10000 \n",
    "eta = 0.0001\n",
    "T = 10\n",
    "M = 1000 # number of stocks to sell\n",
    "# additionaly we take price range to be in between 0 to 100.\n",
    "\n",
    "# Step 1: Initizalize values of Vt(St) for all states St\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
